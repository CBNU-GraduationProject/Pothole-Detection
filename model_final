from flask import Flask, render_template, Response
import cv2
import numpy as np
import os
from tflite_runtime.interpreter import Interpreter, load_delegate
from PIL import Image
import threading

modelPath = os.path.join(os.getcwd(), "ml", "models", "efficientnet_edgetpu.tflite")

tpu_lock = threading.Lock()

app = Flask(__name__)

def create_interpreter():
    interpreter = Interpreter(modelPath, experimental_delegates=[load_delegate('libedgetpu.so.1')])
    interpreter.allocate_tensors()
    return interpreter

def preprocess_image(image, input_details):
    target_size = (input_details[0]['shape'][1], input_details[0]['shape'][2])
    image = image.resize(target_size)
    image_np = np.array(image, dtype=np.uint8)
    image_np = np.expand_dims(image_np, axis=0)  # 차원 확장
    return image_np

def classify_image(interpreter, image_np):
    with tpu_lock:
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        interpreter.set_tensor(input_details[0]['index'], image_np)
        interpreter.invoke()
        output_data = interpreter.get_tensor(output_details[0]['index'])
    return output_data

def async_infer(img_path):

    current_time = time.time()
    if current_time - last_submission_time >= 2:  # 2초 경과 확인
        for prediction in result.get('predictions', []):
            if prediction['confidence'] >= 0.47:
                # Pothole detected, submit form with image
                submit_form({
                    'hazardType': hazard_info,
                    'dates': datetime.now().strftime("%Y-%m-%d"),
                    'hazardImage': img_path,
                    'gps': gps_info,
                    'state': '미조치'
                })
                # Update the last submission time
                last_submission_time = current_time
                break  # Once a submission is made, stop further checks

@app.route('/')
def index():
    return render_template('index.html')

def get_frame():
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    interpreter = create_interpreter()

    while True:
        _, frame = cap.read()
        frameRGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        framePIL = Image.fromarray(frameRGB)
        
        input_details = interpreter.get_input_details()
        framePIL = preprocess_image(framePIL, input_details)

        classify_result = classify_image(interpreter, framePIL)

        predicted_class = np.argmax(classify_result)
        label = LABELS.get(predicted_class, 'Unknown')

        cv2.rectangle(frame, (50, 50), (590, 430), (0, 0, 255), 2)
        cv2.putText(frame, label, (70, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)
        
        imgencode = cv2.imencode('.jpg', frame)[1]
        stringData = imgencode.tobytes()
        yield (b'--frame\r\n'
               b'Content-Type: text/plain\r\n\r\n' + stringData + b'\r\n')
    del(cap)

@app.route('/calc')
def calc():
    return Response(get_frame(),
                    mimetype='multipart/x-mixed-replace; boundary=frame')

if __name__ == '__main__':
    app.run(host='0.0.0.0', debug=True, threaded=True)
